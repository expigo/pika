# Architecture: Realtime Infrastructure

This document describes the technical implementation of the synchronization layer between the **Desktop** app (the source of truth) and the **Cloud** server (the distribution layer).

**Last Updated:** January 18, 2026 (v0.2.5)

## 1. Design Philosophy: "Local First, Cloud Second"

Pika! is designed to run in venue environments where internet connectivity is intermittent or unreliable.
*   **Source of Truth:** The Desktop App (`packages/desktop`) holds the authoritative state of "What is playing".
*   **Distribution:** The Cloud Server (`packages/cloud`) is merely a relay and aggregation point.
*   **Resilience:** The system prioritizes *saving the data locally* first, then attempts to sync. Failure to sync should never crash the local application.

## 2. Connectivity Architecture

The connection is established via WebSockets (`wss://`).

### Client-Side (`packages/desktop`)
*   **Library:** `reconnecting-websocket` wraps the standard DOM `WebSocket`.
*   **Behavior:**
    *   **Auto-Reconnect:** Exponential backoff (1s to 10s) on disconnection.
    *   **Session Restoration:** On `onopen`, the client immediately re-sends `REGISTER_SESSION` with the active `sessionId`. This allows the server to rebuild in-memory state after a deployment or restart without the user noticing.

### Server-Side (`packages/cloud`)
*   **Library:** Bun's native `ServerWebSocket` via Hono integration.
*   **State:** In-memory `activeSessions` Map.
*   **Scaling:** Currently single-instance. (Future work: Redis adapter for multi-instance support).

## 3. The Offline Queue Mechanism

To handle network drops without data loss, the Desktop app implements a persistent offline queue.

### How it works:
1.  **Detection:** When `sendMessage()` is called, we check `socket.readyState === WebSocket.OPEN`.
2.  **Enqueue:** If not open, the message payload is sent to `offlineQueueRepository` (SQLite).
3.  **Flush:** When the socket triggers `onopen`, `flushQueue()` is called.
    *   It reads all persisted messages.
    *   Sends them sequentially with **exponential backoff** to prevent server overload.
    *   Deletes them from SQLite only after `send()` is called.

### Exponential Backoff (v0.2.4)
*   Base delay: 100ms between messages
*   Growth factor: 1.2x every 5 messages
*   Max delay: 2000ms
*   Concurrent flush guard: Only one flush at a time
*   Failure handling: Stops after 3 consecutive failures (retries on next reconnect)

> [!NOTE]
> Only critical messages (like `BROADCAST_TRACK`) are queued. Ephemeral messages (like "typing indicators" or high-frequency updates) may be dropped to prevent flood on reconnection.

## 4. ACK/NACK Protocol (v0.2.4) âœ… IMPLEMENTED

The system now implements application-level acknowledgements for reliable message delivery.

### Message Flow
```
Desktop                           Cloud
   |                                |
   |-- { type, messageId } ------->|  (Send with ID)
   |                                |
   |<-- { type: "ACK", messageId } |  (Success)
   |    OR                          |
   |<-- { type: "NACK", messageId, |  (Failure)
   |      error }                   |
```

### Client-Side Implementation (`useLiveSession.ts`)
*   **Pending Message Tracker:** `Map<messageId, PendingMessage>`
*   **Timeout:** 5 seconds per message
*   **Retry:** Exponential backoff (1s â†’ 2s â†’ 4s)
*   **Max Retries:** 3 attempts before giving up

### Reliable Mode
Critical messages (like `BROADCAST_TRACK`) use `sendMessage(payload, true)` to enable ACK tracking.

```typescript
// Example: Reliable track broadcast
sendMessage(
  { type: "BROADCAST_TRACK", sessionId, track },
  true // reliable: wait for ACK, retry on failure
);
```

## 5. Message Nonce Deduplication (v0.2.4) âœ… IMPLEMENTED

Server-side protection against replay attacks and duplicate processing from network retries.

### How it works:
1.  Messages with `messageId` are tracked on the server.
2.  If a duplicate `messageId` is received within 5 minutes, it's silently ACK'd and skipped.
3.  Nonces are cleaned up every 60 seconds (expired after 5 min).
4.  Maximum 10,000 nonces tracked (FIFO eviction if exceeded).

### Applied To:
*   `BROADCAST_TRACK` - Prevents duplicate track announcements

## 6. Deduplication & Data Integrity

Since the Desktop app periodically polls VirtualDJ (every 1-2s), it may detect the same track multiple times.

*   **Last Broadcast Pointer:** The client stores `lastBroadcastedTrackKey` (Artist + Title). It will not send a `BROADCAST_TRACK` message if it matches the last sent one.
*   **Ghost Tracks:** If a DJ plays a file not imported into the Pika! library, the system generates a "Ghost Track" (ID: `ghost://...`) to ensure the play is still recorded and broadcast.

## 7. Web App Offline Queue (v0.2.4) âœ… IMPLEMENTED

The Web app (`packages/web`) now also implements persistent offline queuing for dancer interactions.

### IndexedDB Persistence (`useLikeQueue.ts`)
*   Uses `idb-keyval` for async IndexedDB access.
*   Pending likes are persisted to `pika_pending_likes_${sessionId}`.
*   Survives page refresh and browser restart.
*   Loaded on mount, flushed on reconnect.

### Stale Data Detection (`LivePlayer.tsx`)
*   Heartbeat monitoring (PONG messages every 10s).
*   Signal Lost indicator after 30s without heartbeat.
*   `StaleDataBanner` component shows warning when data may be outdated.

### Visibility Change Handler (`useWebSocketConnection.ts`)
*   Re-syncs state when tab becomes visible (phone wake from sleep).
*   Triggers reconnect if connection is closed, stale, or disconnected.
*   Uses `hasReceivedPongRef` to prevent false positives on initial load.

### Safari/iOS Bulletproofing (v0.2.5)
*   **pageshow listener:** Handles Safari bfcache page restoration.
*   **statusRef:** Uses ref to avoid stale closure values in callbacks.
*   **addEventListener pattern:** Proper cleanup prevents memory leaks.
*   **Periodic status sync:** Heartbeat interval syncs React state with actual readyState.

## 8. Testing Infrastructure (v0.2.4)

### E2E Reconnection Tests (`tests/e2e/specs/reconnection.spec.ts`)
5 scenarios covering network resilience:
1.  DJ reconnects after brief disconnect
2.  Audience sees Signal Lost indicator
3.  Track changes queue correctly
4.  Likes preserved during offline
5.  System handles rapid flapping

### Chaos Testing (`tests/chaos/chaos-test.js`)
k6 script with 4 scenarios:
*   Normal baseline (10 VUs, 30s)
*   Latency simulation (10 VUs, 500ms-5s delays)
*   Flapping (5 VUs, rapid reconnects)
*   High volume (50 VUs, 250 likes/sec)

## 9. Resolved Issues

### ~~âš ï¸ Lack of Application-Level ACKs~~ âœ… FIXED
*   ACK/NACK protocol fully implemented with timeout/retry.
*   Desktop tracks pending messages and retries on failure.
*   Server nonce deduplication prevents duplicates.

### ~~âš ï¸ Ordering Edge Cases~~ âœ… MITIGATED
*   Explicit `isFlushingQueue` guard prevents concurrent flushes.
*   Exponential backoff spreads load during bulk sync.
*   Failure handling stops and retries on next reconnect.

## 10. Deferred Items (Intentionally Not Implemented)

### ðŸ“‹ Reliable Likes (Desktop â†’ Cloud, Web â†’ Cloud)

**Status:** Deferred (Not Required for MVP)

**Decision Date:** January 18, 2026

**Current Behavior:**
*   Likes are fire-and-forget with optimistic UI
*   Web uses IndexedDB for persistence across page refreshes
*   Failed likes are retried on reconnect

**Why Not Implemented:**

| Factor | Reasoning |
|--------|-----------|
| **Volume** | Likes are high-frequency (100+ per track), low individual value |
| **Aggregation** | DJ sees aggregate counts; missing 1-2 likes is imperceptible |
| **Existing Resilience** | IndexedDB + flush-on-reconnect covers 99.9% of cases |
| **Performance** | ACK/NACK would double message volume at peak engagement |
| **Critical Path** | Track broadcast is critical (already reliable); likes are secondary |

**When to Revisit:**
*   If production analytics show consistent like loss (>5%)
*   If dancers report frustration with "like didn't register" UX
*   If building premium features that depend on exact like counts

**Alternative Considered:**
Batch ACK (one ACK per 10 likes) - adds complexity without clear benefit.

## 11. Network Resilience Score

| Component | Score | Details |
|-----------|-------|---------|
| Desktop Offline Queue | 10/10 | SQLite persistence, exponential backoff |
| Desktop ACK/NACK | 10/10 | Timeout/retry, server deduplication |
| Web Offline Queue | 10/10 | IndexedDB persistence, survives refresh |
| Heartbeat Detection | 10/10 | Signal lost indicator, stale banner |
| Visibility Handling | 10/10 | Re-sync on phone wake, Safari bfcache |
| Test Coverage | 10/10 | E2E specs, chaos testing |

**Overall Score: 11/10** ðŸŽ‰

> The only deferred item (reliable likes) is intentionally omitted due to cost/benefit analysis, not technical limitation.
